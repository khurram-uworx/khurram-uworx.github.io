---
title: "The Best Way to Prevent Cache Stampedes"
date: 2026-01-07
tags:
    - dotnet
    - caching
comments: true

---

In **2026 / .NET 10**, the *most elegant* and idiomatic solution to the **cache stampede (dogpile) problem** in C# is:

> **Use built-in ‚Äúsingle-flight‚Äù caching via `HybridCache.GetOrCreateAsync`**
> (or `IMemoryCache.GetOrCreateAsync` + async locking if HybridCache isn‚Äôt available)

This gives you **correctness, simplicity, and async safety** without reinventing concurrency primitives.

---

## 1Ô∏è‚É£ The Modern Best Practice: `HybridCache` (Recommended)

### Why this is the elegant solution

`Microsoft.Extensions.Caching.Hybrid` (introduced in .NET 8 and mature by .NET 10):

* ‚úÖ Built-in **cache stampede protection**
* ‚úÖ Ensures **only one factory runs per key**
* ‚úÖ Async-safe, cancellation-aware
* ‚úÖ Supports **L1 memory + L2 distributed cache**
* ‚úÖ No custom locking code

### Example

```csharp
public async Task<Product> GetProductAsync(
    int id,
    CancellationToken ct)
{
    return await _cache.GetOrCreateAsync(
        $"product:{id}",
        async entry =>
        {
            entry.AbsoluteExpirationRelativeToNow = TimeSpan.FromMinutes(5);
            return await _repository.GetProductAsync(id, ct);
        },
        ct);
}
```

### What happens under load?

* 1,000 concurrent requests hit a cold cache
* **Only one request** executes `_repository.GetProductAsync`
* Others **await the same Task**
* No lock contention, no duplicated DB calls

‚úî This is *true single-flight caching*

---

## 2Ô∏è‚É£ If You‚Äôre Using Only `IMemoryCache`

`IMemoryCache.GetOrCreateAsync` **also prevents stampedes** as long as:

* You **only cache the result**
* You **don‚Äôt do external locking**
* Your factory is fully async

### Correct pattern

```csharp
public Task<User> GetUserAsync(Guid id)
{
    return _cache.GetOrCreateAsync(
        $"user:{id}",
        async entry =>
        {
            entry.AbsoluteExpirationRelativeToNow = TimeSpan.FromMinutes(2);
            return await LoadUserAsync(id);
        });
}
```

### Why this works

Internally, `MemoryCache`:

* Locks per key
* Shares the in-flight Task
* Releases waiters once completed

‚ö†Ô∏è **Common mistake**

```csharp
if (!_cache.TryGetValue(key, out var value))
{
    value = await LoadAsync(); // ‚ùå stampede risk
    _cache.Set(key, value);
}
```

---

## 3Ô∏è‚É£ Advanced / Custom: `AsyncLazy<T>` or `Task<T>` Caching

If you need **full control**, the classic single-flight pattern still works:

```csharp
private readonly ConcurrentDictionary<string, Lazy<Task<T>>> _inflight = new();

public Task<T> GetAsync(string key)
{
    var lazy = _inflight.GetOrAdd(
        key,
        _ => new Lazy<Task<T>>(LoadAsync));

    return lazy.Value.ContinueWith(t =>
    {
        _inflight.TryRemove(key, out _);
        return t.Result;
    });
}
```

### Use this only if:

* You need **non-expiring inflight coordination**
* You‚Äôre implementing a **custom cache**
* You need behavior not supported by `HybridCache`

Otherwise, this is **more code than value** in 2025.

---

## 4Ô∏è‚É£ What *Not* to Use Anymore ‚ùå

| Pattern                 | Why                      |
| ----------------------- | ------------------------ |
| Global `SemaphoreSlim`  | Kills throughput         |
| Per-key `SemaphoreSlim` | Hard to manage lifecycle |
| Double-check locking    | Error-prone with async   |
| Polly-only locks        | Not caching-aware        |
| Manual Redis locks      | Overkill for read caches |

---

## ‚úÖ Final Recommendation (2025)

### ü•á Best Overall

**`HybridCache.GetOrCreateAsync`**

> Clean, safe, scalable, async-first, future-proof

### ü•à Good Enough

**`IMemoryCache.GetOrCreateAsync`**

### ü•â Only If Necessary

**Custom single-flight via `Lazy<Task<T>>`**

---

If you **must implement it yourself** in 2025‚Äîno `IMemoryCache`, no `HybridCache`, no ‚Äúfancy‚Äù caching libraries‚Äîthe **most elegant, correct, and idiomatic C# solution** is a **single-flight async cache** using:

> **`ConcurrentDictionary<TKey, Task<TValue>>` (or `Lazy<Task<T>>`)**

This is the simplest design that:

* Prevents cache stampede
* Is fully async-safe
* Requires no locks
* Scales under high concurrency
* Is easy to reason about

---

## ‚úÖ The Canonical DIY Solution (Recommended)

### Core idea

Cache **the in-flight Task**, not the result.

All concurrent callers await the same task.

---

### Minimal, correct implementation

```csharp
public sealed class SingleFlightCache<TKey, TValue>
    where TKey : notnull
{
    private readonly ConcurrentDictionary<TKey, Task<TValue>> _tasks = new();

    public async Task<TValue> GetAsync(
        TKey key,
        Func<CancellationToken, Task<TValue>> factory,
        CancellationToken ct = default)
    {
        var task = _tasks.GetOrAdd(key, _ =>
            CreateTask(key, factory, ct));

        try
        {
            return await task.ConfigureAwait(false);
        }
        catch
        {
            // Important: do not cache failures
            _tasks.TryRemove(key, out _);
            throw;
        }
    }

    private async Task<TValue> CreateTask(
        TKey key,
        Func<CancellationToken, Task<TValue>> factory,
        CancellationToken ct)
    {
        try
        {
            return await factory(ct).ConfigureAwait(false);
        }
        finally
        {
            // Remove only after completion
            _tasks.TryRemove(key, out _);
        }
    }
}
```

---

## üß† Why this is elegant

### ‚úî No locks

`ConcurrentDictionary` already gives you atomicity.

### ‚úî No `SemaphoreSlim`

Avoids contention and deadlocks.

### ‚úî Natural async behavior

Callers just `await`.

### ‚úî Automatic stampede prevention

First caller executes `factory`, others await.

### ‚úî Failure-safe

Exceptions are **not cached**.

---

## ‚ö†Ô∏è Critical Design Details (Most People Miss These)

### 1Ô∏è‚É£ **Do NOT cache failed Tasks**

Otherwise one failure poisons all future callers.

‚úî Remove the entry on exception.

---

### 2Ô∏è‚É£ **Do NOT pass per-request CancellationTokens**

If one caller cancels, all callers cancel.

‚ùå Bad:

```csharp
GetAsync(key, factory, requestCt)
```

‚úî Good:

```csharp
GetAsync(key, factory, CancellationToken.None)
```

Or:

```csharp
using var cts = CancellationTokenSource
    .CreateLinkedTokenSource(appShutdownToken);
```

---

### 3Ô∏è‚É£ **Don‚Äôt remove before completion**

Removing too early causes double execution.

‚ùå Wrong:

```csharp
_tasks.TryRemove(key, out _); // before await
```

‚úî Correct:

```csharp
finally { _tasks.TryRemove(key, out _); }
```

---

## üîÅ Variant: `Lazy<Task<T>>` (Stronger Guarantees)

If you want **exactly-once execution**, even under extreme races:

```csharp
private readonly ConcurrentDictionary<TKey, Lazy<Task<TValue>>> _cache = new();

public Task<TValue> GetAsync(TKey key, Func<Task<TValue>> factory)
{
    var lazy = _cache.GetOrAdd(
        key,
        _ => new Lazy<Task<TValue>>(
            factory,
            LazyThreadSafetyMode.ExecutionAndPublication));

    return lazy.Value.ContinueWith(t =>
    {
        _cache.TryRemove(key, out _);
        return t.Result;
    });
}
```

### When to use this

* You absolutely must guarantee only one factory invocation
* You accept slightly more complexity

---

## ‚ùå Patterns to Avoid (Even DIY)

| Pattern               | Problem                          |
| --------------------- | -------------------------------- |
| `lock`                | Blocks threads                   |
| `SemaphoreSlim`       | Easy to leak / forget to release |
| Double-check locking  | Broken with async                |
| Manual spinlocks      | CPU waste                        |
| One semaphore per key | Lifecycle hell                   |

---

## üß™ Bonus: Add TTL (Still Simple)

```csharp
record CacheEntry(Task<TValue> Task, DateTimeOffset Expiry);

private readonly ConcurrentDictionary<TKey, CacheEntry> _cache = new();

public Task<TValue> GetAsync(TKey key, Func<Task<TValue>> factory)
{
    var now = DateTimeOffset.UtcNow;

    var entry = _cache.AddOrUpdate(
        key,
        _ => new CacheEntry(factory(), now.AddMinutes(5)),
        (_, old) =>
            old.Expiry > now ? old : new CacheEntry(factory(), now.AddMinutes(5)));

    return entry.Task;
}
```

‚úî Still stampede-safe
‚úî Still async
‚úî No locks

---

Great question. This is where the **design philosophies really diverge**, even though they solve the same *cache stampede* problem.

Below is a **clear, practical comparison** of:

* **.NET single-flight caching** (HybridCache / IMemoryCache / Task-based)
* **Java‚Äôs Caffeine**
* **Go‚Äôs `singleflight`**

with emphasis on **mechanics, guarantees, and ergonomics** rather than marketing.

---

## 1Ô∏è‚É£ Conceptual Model (One-Line Summary)

| Platform                | Mental Model                              |
| ----------------------- | ----------------------------------------- |
| **.NET**                | *Cache the in-flight `Task<T>`*           |
| **Java (Caffeine)**     | *Block competing threads on the same key* |
| **Go (`singleflight`)** | *Deduplicate concurrent function calls*   |

---

## 2Ô∏è‚É£ Core API Comparison

### .NET (HybridCache / IMemoryCache)

```csharp
await cache.GetOrCreateAsync(key, async entry =>
{
    return await LoadAsync();
});
```

### Java (Caffeine)

```java
cache.get(key, k -> load(k));
```

### Go (singleflight)

```go
v, err, _ := group.Do(key, func() (any, error) {
    return load()
})
```

---

## 3Ô∏è‚É£ Concurrency Mechanics (Critical Differences)

### üü¶ .NET ‚Äî *Task sharing (async-first)*

* First caller starts `Task<T>`
* Other callers **await the same Task**
* No threads blocked
* Naturally async & scalable

```text
Requests ‚îÄ‚îÄ‚ñ∫ same Task<T> ‚îÄ‚îÄ‚ñ∫ await
```

‚úî **Best fit for async I/O**

---

### üü® Java (Caffeine) ‚Äî *Thread blocking*

* First thread computes value
* Other threads **block** on the same key
* Uses locks / condition variables

```text
Threads ‚îÄ‚îÄ‚ñ∫ lock ‚îÄ‚îÄ‚ñ∫ wait ‚îÄ‚îÄ‚ñ∫ wake
```

‚úî Very efficient for **CPU-bound work**
‚ùå Less ideal for async/reactive models

---

### üü• Go ‚Äî *Goroutine deduplication*

* Goroutines wait on channels
* No OS thread blocking
* Explicitly **not a cache**

```text
goroutines ‚îÄ‚îÄ‚ñ∫ channel wait ‚îÄ‚îÄ‚ñ∫ resume
```

‚úî Extremely lightweight
‚úî Explicit failure semantics
‚ùå No TTL, eviction, or storage

---

## 4Ô∏è‚É£ Failure & Cancellation Semantics

| Aspect            | .NET                          | Caffeine            | Go singleflight |
| ----------------- | ----------------------------- | ------------------- | --------------- |
| Exception cached? | ‚ùå No (unless you do it wrong) | ‚ùå No                | ‚ùå No            |
| Cancellation      | Shared Task ‚Üí risky           | Thread interruption | Context-based   |
| Retry on failure  | Automatic                     | Automatic           | Manual          |
| Partial success   | Possible                      | No                  | No              |

### Subtle .NET gotcha

Passing a **request-scoped `CancellationToken`** can cancel *everyone*.

‚úî Best practice:

```csharp
CancellationToken.None
```

---

## 5Ô∏è‚É£ TTL & Eviction

| Feature             | .NET        | Caffeine        | Go |
| ------------------- | ----------- | --------------- | -- |
| TTL                 | Yes         | Yes             | ‚ùå  |
| Size-based eviction | HybridCache | Yes (W-TinyLFU) | ‚ùå  |
| Background refresh  | Manual      | Built-in        | ‚ùå  |
| Multi-level cache   | Yes         | No              | ‚ùå  |

üèÜ **Caffeine wins eviction sophistication**
üèÜ **.NET wins multi-layer caching**
üèÜ **Go keeps things orthogonal**

---

## 6Ô∏è‚É£ Performance Characteristics

### Latency under stampede

| Platform   | Result                   |
| ---------- | ------------------------ |
| .NET async | Lowest                   |
| Go         | Near-zero                |
| Java       | Higher (blocked threads) |

### Throughput under load

| Platform | Behavior               |
| -------- | ---------------------- |
| .NET     | Scales with async I/O  |
| Java     | Limited by thread pool |
| Go       | Massive concurrency    |

---

## 7Ô∏è‚É£ Elegance vs Explicitness

| Dimension        | .NET           | Java              | Go                 |
| ---------------- | -------------- | ----------------- | ------------------ |
| Boilerplate      | Very low       | Low               | Medium             |
| Explicit control | Medium         | Low               | High               |
| Footguns         | Cancellation   | Thread exhaustion | Forgetting cache   |
| Philosophy       | Runtime-driven | Library-driven    | Minimal primitives |

---

## 8Ô∏è‚É£ Design Philosophy Comparison

### .NET

> *‚ÄúAsync is the primitive.‚Äù*

Single-flight emerges naturally from `Task<T>`.

---

### Java

> *‚ÄúThreads are the primitive.‚Äù*

Single-flight implemented via blocking + eviction heuristics.

---

### Go

> *‚ÄúCoordination, not caching.‚Äù*

`singleflight` intentionally avoids policy.

---

## 9Ô∏è‚É£ Who Wins What?

| Use Case                        | Winner      |
| ------------------------------- | ----------- |
| Async microservices             | üü¶ .NET     |
| High-throughput in-memory cache | üü® Caffeine |
| Lightweight RPC deduplication   | üü• Go       |
| Multi-tier caching              | üü¶ .NET     |
| Explicit control                | üü• Go       |

---

## üèÅ Final Verdict

> **The most elegant single-flight implementation depends on the runtime model:**

* **.NET** ‚Üí *Task-based caching is the cleanest*
* **Java** ‚Üí *Caffeine is unmatched for eviction*
* **Go** ‚Üí *`singleflight` is minimal and perfect*

There is **no universal winner**, only alignment with the language‚Äôs concurrency philosophy.

If we‚Äôre handing out the ‚ÄúSingle-Flight Oscar‚Äù üèÜ based purely on **elegance, safety, and developer ergonomics in 2025**, the winner is:

### **ü•á .NET (Task-based single-flight caching)**

Here‚Äôs why it takes the gold:

---

### 1Ô∏è‚É£ Async-first by design

* No threads blocked
* Works naturally with `async/await`
* Fits modern microservices and I/O-heavy workloads

### 2Ô∏è‚É£ Minimal boilerplate

* `GetOrCreateAsync` + `Task<T>` sharing = **no locks, no semaphores**
* Failure-safe and exception-aware
* Cancellation-aware if done right

### 3Ô∏è‚É£ Multi-tier ready

* Supports L1 memory + L2 distributed cache
* Handles cache eviction gracefully
* Plays well with background refresh / stale-while-revalidate

### 4Ô∏è‚É£ Low footgun risk

* No deadlocks
* No thread pool exhaustion
* Natural single-flight semantics

---

### ü•à Silver: Go `singleflight`

* Brilliantly minimal and explicit
* Extremely lightweight under high concurrency
* But it‚Äôs **not a cache**; TTL, eviction, multi-layer caching all have to be added manually

### ü•â Bronze: Java Caffeine

* Very fast and sophisticated for CPU-bound caches
* Built-in eviction policies like W-TinyLFU
* But it **blocks threads**, which makes async/reactive systems less elegant

---

üí° **TL;DR**:

> .NET wins the **‚Äúmost elegant, production-ready single-flight caching‚Äù Oscar** in 2025.

It‚Äôs async-native, safe, concise, and multi-tier ready‚Äîbasically what other languages are still hacking around to replicate.

---
